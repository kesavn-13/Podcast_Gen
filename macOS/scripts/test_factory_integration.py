"""
Test the client factory system integration
Shows how Google LLM integration works in your existing architecture
"""

import os
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

def test_client_factory():
    """Test the enhanced client factory system"""
    
    print("ğŸ§ª TESTING CLIENT FACTORY INTEGRATION")
    print("=" * 50)
    
    # Test different configurations
    configurations = [
        {
            "name": "Google Gemini (requires API key)",
            "env": {
                'USE_GOOGLE_LLM': 'true',
                'USE_LOCAL_LLM': 'false', 
                'USE_MOCK_SERVICES': 'false',
                'GOOGLE_API_KEY': 'test-key-here'
            }
        },
        {
            "name": "Local LLM (Ollama)",
            "env": {
                'USE_GOOGLE_LLM': 'false',
                'USE_LOCAL_LLM': 'true',
                'USE_MOCK_SERVICES': 'false'
            }
        },
        {
            "name": "Mock Clients (Development)", 
            "env": {
                'USE_GOOGLE_LLM': 'false',
                'USE_LOCAL_LLM': 'false',
                'USE_MOCK_SERVICES': 'true'
            }
        }
    ]
    
    for config in configurations:
        print(f"\nğŸ”§ Testing: {config['name']}")
        print("-" * 30)
        
        # Set environment variables
        for key, value in config['env'].items():
            os.environ[key] = value
        
        try:
            # Import and test the factory
            from backend.tools.sm_client import create_clients
            
            reasoning_client, embedding_client = create_clients()
            
            print(f"   âœ… Reasoning client: {type(reasoning_client).__name__}")
            print(f"   âœ… Embedding client: {type(embedding_client).__name__}")
            
            # Test if the client has the expected interface
            if hasattr(reasoning_client, 'generate'):
                print("   âœ… Has generate() method")
            else:
                print("   âŒ Missing generate() method")
                
            if hasattr(embedding_client, 'embed'):
                print("   âœ… Has embed() method")
            else:
                print("   âŒ Missing embed() method")
            
        except Exception as e:
            print(f"   âŒ Failed: {e}")
    
    print("\n" + "=" * 50)
    print("ğŸ¯ CLIENT FACTORY INTEGRATION SUMMARY")
    print("=" * 50)
    print("âœ… Your system now supports multiple LLM backends:")
    print("   1. ğŸ¤– Google Gemini (when API key is provided)")
    print("   2. ğŸ¦™ Local Ollama (when installed and running)")
    print("   3. ğŸ­ Mock clients (for development)")
    print("   4. ğŸš€ NVIDIA NIM (for production)")
    
    print("\nğŸ“‹ Integration Status:")
    print("âœ… Factory function updated with priority system")
    print("âœ… Google LLM client created with compatible interface")
    print("âœ… Environment configuration ready")
    print("âœ… Existing Paperâ†’Podcast system unchanged")
    
    print("\nğŸ”§ To Use Google Gemini:")
    print("   1. Get API key: https://makersuite.google.com/app/apikey")
    print("   2. Update .env: GOOGLE_API_KEY=your-actual-key")
    print("   3. Run: python scripts/test_google_integration.py")


if __name__ == "__main__":
    test_client_factory()