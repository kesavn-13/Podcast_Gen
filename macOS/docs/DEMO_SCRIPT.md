# üé¨ Demo Video Script
**Paper‚ÜíPodcast: Agentic + Verified**  
*AWS & NVIDIA Hackathon Submission*

---

## üìã Demo Overview
**Duration**: 3 minutes maximum  
**Goal**: Showcase complete agentic workflow from paper upload to verified podcast

## üéØ Key Messages to Convey
1. **Agentic Behavior**: Autonomous decision-making, not just prompt chaining
2. **Verification Loop**: Fact-checking with citation-backed content
3. **NVIDIA NIM Integration**: Required hackathon components in action
4. **AWS Infrastructure**: Scalable deployment on SageMaker
5. **Real-World Value**: Transforms dense research into accessible audio

---

## üé• Scene-by-Scene Script

### Scene 1: Introduction (0:00 - 0:30)
**Visual**: Title screen with hackathon logos  
**Narration**:
> "Welcome to Paper‚ÜíPodcast: an agentic AI system that transforms dense research papers into engaging, fact-checked podcast episodes. Built for the AWS & NVIDIA Hackathon using llama-3.1-nemotron-nano-8B-v1 and Retrieval NIM on AWS SageMaker."

**On-Screen Elements**:
- Project logo and title
- "AWS & NVIDIA Hackathon Submission" badge
- "Agentic + Verified" tagline

### Scene 2: Problem Setup (0:30 - 0:45)
**Visual**: Split screen showing dense academic paper vs. commuter with headphones  
**Narration**:
> "Research papers take 45-60 minutes to read, but most audio summaries lack citations and accuracy. We need trustworthy, engaging audio that preserves source fidelity."

**On-Screen Elements**:
- Timer showing "45-60 min reading time"
- Error icons on generic audio summaries
- ‚úÖ checkmarks for "citations" and "accuracy"

### Scene 3: Upload & Indexing (0:45 - 1:15)
**Visual**: Web interface with paper upload  
**Narration**:
> "Here's our agentic workflow in action. I'll upload a research paper on transformer architectures. Watch as the system autonomously processes and indexes the content using dual RAG - both facts and conversational style patterns."

**Actions**:
1. Drag and drop PDF file
2. Show auto-extraction of title, authors, page count
3. Display dual indexing progress: "Facts Index" and "Style Bank"
4. Show state machine progression: Upload ‚Üí Indexing ‚Üí Planning

**On-Screen Elements**:
- File details extraction
- Progress bars for dual indexing
- State machine visualization
- Cost tracker: "$0.50 used"

### Scene 4: Autonomous Planning (1:15 - 1:45)
**Visual**: Episode outline generation in real-time  
**Narration**:
> "The planning agent creates a three-segment episode structure. Notice how it autonomously determines content breakdown, duration targets, and key discussion points - this isn't just following a template."

**Actions**:
1. Show outline generation in progress
2. Display segment breakdown:
   - Intro: "Introduction to Transformers" (60s)
   - Core: "Attention Mechanisms Explained" (300s)  
   - Takeaways: "Real-World Applications" (60s)
3. Show complexity scoring and duration budgeting

**On-Screen Elements**:
- Episode outline with timing
- Complexity meter: 0.7/1.0
- Key points extraction
- State: Planning ‚Üí Drafting

### Scene 5: Script Generation & Fact-Checking (1:45 - 2:15)
**Visual**: Two-column view showing script generation and verification  
**Narration**:
> "Now the real magic: segment-by-segment script generation with inline fact-checking. Watch as the verification agent catches an unsupported claim and triggers an automatic rewrite."

**Actions**:
1. Show script lines appearing with speaker assignments
2. Display real-time fact-checking
3. **Key Demo Moment**: Show line flagged as "[needs source]"
4. Trigger rewrite process
5. Show corrected line with citations

**On-Screen Elements**:
- Script with speaker labels (Host 1, Host 2)
- Citation links appearing
- Verification status: ‚ùå ‚Üí üîÑ ‚Üí ‚úÖ
- "100% Verified" badge when complete

### Scene 6: Audio Generation (2:15 - 2:45)
**Visual**: Audio waveform generation and playback  
**Narration**:
> "With verified content, we generate high-quality audio using Amazon Polly with natural conversation flow. Listen to this 20-second sample of our two-host format."

**Actions**:
1. Show TTS generation progress
2. Display audio waveforms for both voices
3. **Play 20-second audio sample** (most important part!)
4. Show segment stitching with transitions

**Audio Sample Script**:
> **Host 1**: "So let's dive into attention mechanisms - the breakthrough that made transformers possible."
> 
> **Host 2**: "Absolutely! According to Vaswani et al. on page 3, attention allows the model to focus on different parts of the input sequence simultaneously."
> 
> **Host 1**: "That's a game-changer compared to RNNs that process sequentially..."

**On-Screen Elements**:
- Dual voice waveforms
- Citation popup: "Vaswani et al., page 3"
- Audio player controls

### Scene 7: Results & Features (2:45 - 3:00)
**Visual**: Final podcast package and metrics dashboard  
**Narration**:
> "In under 5 minutes, we've transformed a 15-page paper into a verified, engaging podcast. Complete with citations, transcript, and cost tracking. All powered by autonomous agents on AWS infrastructure."

**Actions**:
1. Show final podcast package:
   - MP3 file with chapters
   - Transcript with clickable citations
   - Episode metadata
2. Display metrics:
   - Processing time: 4m 32s
   - Cost: $2.43 / $95 budget
   - Verification rate: 98%
   - Citation count: 12

**On-Screen Elements**:
- Package contents overview
- Metrics dashboard
- Budget remaining: $92.57
- Links to AWS services used

---

## üé® Visual Design Elements

### Color Scheme
- **Primary**: AWS Orange (#FF9900)
- **Secondary**: NVIDIA Green (#76B900) 
- **Accent**: Deep Blue (#232F3E)
- **Success**: Bright Green (#00D4AA)
- **Warning**: Amber (#FFC107)

### Typography
- **Headlines**: Roboto Bold
- **Body**: Roboto Regular
- **Code/Technical**: Fira Code

### UI Components
- State machine visualization (animated)
- Progress bars with smooth animations
- Citation popups with hover effects
- Real-time cost tracker
- Verification checkmarks with animations

---

## üìä Technical Specifications

### Screen Resolution
- **Recording**: 1920x1080 (Full HD)
- **Frame Rate**: 30 FPS
- **Format**: MP4 (H.264)

### Audio Quality
- **Sample Rate**: 48kHz
- **Bit Depth**: 16-bit
- **Format**: AAC
- **Levels**: -12dB peak, -23dB LUFS

### Recording Tools
- **Screen Capture**: OBS Studio or similar
- **Audio**: Clear narration with minimal background noise
- **Editing**: DaVinci Resolve or Adobe Premiere

---

## üéØ Success Metrics

### Must Demonstrate
- ‚úÖ NVIDIA NIM models in action (llama + retrieval)
- ‚úÖ AWS SageMaker deployment
- ‚úÖ Agentic behavior (autonomous decisions)
- ‚úÖ Fact verification with citations
- ‚úÖ Complete workflow (upload ‚Üí podcast)
- ‚úÖ Real audio generation and playback

### Bonus Points
- ‚úÖ Cost monitoring and budget control
- ‚úÖ Segment-level regeneration capability
- ‚úÖ Plain language mode toggle
- ‚úÖ Real-time state machine visualization
- ‚úÖ Citation navigation in transcript

---

## üó£Ô∏è Narration Guidelines

### Tone
- **Professional** but approachable
- **Enthusiastic** about the technology
- **Confident** in capabilities
- **Clear** technical explanations

### Pacing
- **Moderate speed**: Allow time to absorb visuals
- **Strategic pauses**: At key demo moments
- **Emphasis**: On hackathon requirements and unique features

### Key Phrases to Include
- "Agentic behavior" (not just automation)
- "NVIDIA NIM integration"
- "AWS SageMaker deployment" 
- "Fact-verified content"
- "Citation-backed accuracy"
- "Autonomous decision-making"

---

## üé¨ Production Checklist

### Pre-Recording
- [ ] Test all demo flows multiple times
- [ ] Ensure stable AWS infrastructure
- [ ] Prepare sample research paper (5-10 pages)
- [ ] Set up clean browser environment
- [ ] Test audio generation with sample content
- [ ] Verify all API endpoints are responsive

### During Recording
- [ ] Start with introduction slide
- [ ] Follow script timing precisely
- [ ] Demonstrate each key feature clearly
- [ ] Include 20-second audio playback
- [ ] Show real metrics and costs
- [ ] End with results summary

### Post-Recording
- [ ] Edit for smooth transitions
- [ ] Add captions/subtitles
- [ ] Color correct for consistency
- [ ] Audio level balancing
- [ ] Export at competition specifications
- [ ] Upload to required platform (YouTube/Vimeo)

---

## üìù Submission Assets

### Video Files
- **Primary**: `paper_podcast_demo.mp4` (< 180 seconds)
- **Backup**: `paper_podcast_demo_backup.mp4`
- **Thumbnails**: High-quality static images

### Supporting Materials
- **Transcript**: Complete narration text
- **Screenshots**: Key moments for documentation
- **Technical Diagram**: Architecture overview
- **Cost Breakdown**: Detailed AWS usage

---

## üöÄ Contingency Plans

### Technical Issues
- **Slow API response**: Pre-record segments if needed
- **Audio generation fails**: Have backup audio files
- **Infrastructure down**: Use local mock services
- **Network issues**: Record offline components separately

### Content Alternatives
- **Different paper**: Have 2-3 backup research papers ready
- **Shorter demo**: Focus on core agentic features if time runs short
- **Technical difficulties**: Emphasize architecture over live demo

---

**üéØ Remember**: This demo needs to wow the judges with genuine agentic behavior, real NVIDIA NIM integration, and practical value. Show, don't just tell!