"""
Complete End-to-End Agent Workflow Test
Tests the full pipeline: upload → index → outline → segments → factcheck → rewrite → TTS → stitch
Can run with mocks, local LLM, or real NVIDIA NIM
"""

import asyncio
import os
import sys
import json
from pathlib import Path
from typing import Dict, Any, List
import logging

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from backend.tools.sm_client import create_clients
from rag.indexer import RetrievalInterface
from app.audio_generator import create_audio_producer

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)


class PodcastAgentOrchestrator:
    """
    Main orchestrator for the agentic podcast generation workflow
    Manages the complete pipeline from paper to final episode
    """
    
    def __init__(self, use_local_llm: bool = False):
        self.use_local_llm = use_local_llm
        
        # Initialize clients
        if use_local_llm:
            try:
                from backend.tools.local_llm_client import create_local_clients
                self.reasoning_client, self.embedding_client = create_local_clients()
                logger.info("🦙 Using local LLM (Ollama) clients")
            except ImportError:
                logger.warning("⚠️  Local LLM not available, falling back to mocks")
                self.reasoning_client, self.embedding_client = create_clients()
        else:
            self.reasoning_client, self.embedding_client = create_clients()
            logger.info("🎭 Using mock NIM clients")
        
        # Initialize RAG system
        self.retrieval = RetrievalInterface(use_local=True)
        
        # Initialize audio producer
        self.audio_producer = create_audio_producer(use_aws=False)
        
        # Episode state
        self.episodes = {}
    
    async def process_paper(self, paper_id: str, paper_path: str) -> Dict[str, Any]:
        """
        Complete paper processing workflow
        
        Args:
            paper_id: Unique identifier for the paper
            paper_path: Path to the paper file
            
        Returns:
            Complete episode metadata
        """
        logger.info(f"📄 Starting paper processing: {paper_id}")
        
        # Step 1: Load and index paper
        paper_content = await self._load_paper(paper_path)
        if not paper_content:
            return {"error": "Failed to load paper"}
        
        # Step 2: Index for RAG
        index_success = await self.retrieval.index_paper(
            paper_id=paper_id,
            content=paper_content['content'],
            title=paper_content['title']
        )
        
        if not index_success:
            return {"error": "Failed to index paper"}
        
        # Step 3: Generate episode outline
        outline = await self._generate_outline(paper_id, paper_content)
        
        # Step 4: Process each segment
        segments = []
        for i, segment_info in enumerate(outline.get('segments', []), 1):
            segment = await self._process_segment(paper_id, i, segment_info)
            segments.append(segment)
        
        # Step 5: Generate final episode
        episode_audio = await self._generate_episode_audio(paper_id, segments)
        
        # Create episode metadata
        episode = {
            "paper_id": paper_id,
            "title": paper_content['title'],
            "outline": outline,
            "segments": segments,
            "audio_path": episode_audio,
            "status": "completed",
            "factuality_score": self._calculate_factuality(segments),
            "total_duration": sum(s.get('duration', 0) for s in segments)
        }
        
        self.episodes[paper_id] = episode
        logger.info(f"✅ Episode completed: {paper_id}")
        return episode
    
    async def _load_paper(self, paper_path: str) -> Dict[str, Any]:
        """Load paper content from file"""
        try:
            with open(paper_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Extract title (first line or from filename)
            lines = content.strip().split('\n')
            title = lines[0] if lines else Path(paper_path).stem
            
            return {
                "title": title,
                "content": content,
                "path": paper_path
            }
        except Exception as e:
            logger.error(f"❌ Failed to load paper: {e}")
            return None
    
    async def _generate_outline(self, paper_id: str, paper_content: Dict[str, Any]) -> Dict[str, Any]:
        """Generate episode outline using RAG + LLM"""
        logger.info(f"📋 Generating outline for {paper_id}")
        
        # Get relevant facts from RAG
        facts = await self.retrieval.retrieve_facts(
            "main concepts findings methodology results", 
            k=5, 
            paper_id=paper_id
        )
        
        # Get style patterns
        styles = await self.retrieval.retrieve_styles(
            "episode structure conversation flow", 
            k=2
        )
        
        # Build context
        facts_context = "\n".join([f"- {fact['text'][:200]}" for fact in facts])
        style_context = "\n".join([f"Style: {style['text'][:100]}" for style in styles])
        
        # Generate outline
        messages = [
            {
                "role": "system",
                "content": """You are a podcast producer creating episode outlines. Generate a JSON response with this structure:
{
  "episode_title": "string",
  "target_duration_minutes": 15,
  "segments": [
    {
      "title": "string",
      "target_duration": 180,
      "key_points": ["point1", "point2"],
      "speaker_focus": "host1|host2|both"
    }
  ]
}"""
            },
            {
                "role": "user", 
                "content": f"""Create a podcast episode outline for this research paper:

Title: {paper_content['title']}

Key Facts from RAG:
{facts_context}

Style Guidelines:
{style_context}

Create 3-4 segments totaling ~15 minutes for an engaging, educational podcast."""
            }
        ]
        
        response = await self.reasoning_client.generate(messages, response_type="outline")
        
        if response and 'choices' in response:
            outline_content = response['choices'][0]['message']['content']
            if isinstance(outline_content, dict):
                return outline_content
            else:
                # Parse if string response
                return {"segments": [{"title": "Generated Outline", "target_duration": 300}]}
        
        # Fallback outline
        return {
            "episode_title": f"Exploring {paper_content['title']}",
            "target_duration_minutes": 15,
            "segments": [
                {"title": "Introduction", "target_duration": 180, "key_points": ["Paper overview"]},
                {"title": "Methodology", "target_duration": 240, "key_points": ["Research approach"]},
                {"title": "Results", "target_duration": 240, "key_points": ["Key findings"]},
                {"title": "Implications", "target_duration": 180, "key_points": ["Impact and future work"]}
            ]
        }
    
    async def _process_segment(self, paper_id: str, segment_num: int, segment_info: Dict[str, Any]) -> Dict[str, Any]:
        """Process a single segment: generate → factcheck → rewrite → TTS"""
        logger.info(f"🎬 Processing segment {segment_num}: {segment_info.get('title', 'Untitled')}")
        
        # Generate segment script
        segment_script = await self._generate_segment_script(paper_id, segment_num, segment_info)
        
        # Fact-check the script
        factcheck_result = await self._factcheck_segment(paper_id, segment_script)
        
        # Rewrite if needed
        if factcheck_result.get('needs_rewrite', False):
            segment_script = await self._rewrite_segment(paper_id, segment_script, factcheck_result)
        
        # Generate audio
        audio_path = await self._generate_segment_audio(paper_id, segment_num, segment_script)
        
        return {
            "segment_num": segment_num,
            "title": segment_info.get('title', f'Segment {segment_num}'),
            "script": segment_script,
            "factcheck": factcheck_result,
            "audio_path": audio_path,
            "duration": segment_info.get('target_duration', 180)
        }
    
    async def _generate_segment_script(self, paper_id: str, segment_num: int, segment_info: Dict[str, Any]) -> List[Dict[str, str]]:
        """Generate conversation script for a segment"""
        # Get relevant facts for this segment
        segment_query = " ".join(segment_info.get('key_points', ['general discussion']))
        facts = await self.retrieval.retrieve_facts(segment_query, k=3, paper_id=paper_id)
        
        # Get conversation style
        styles = await self.retrieval.retrieve_styles("conversational patterns", k=1)
        
        # Build context
        facts_context = "\n".join([f"[Source p.{i+1}] {fact['text'][:150]}" for i, fact in enumerate(facts)])
        
        messages = [
            {
                "role": "system",
                "content": """Generate podcast conversation script. Return JSON:
{
  "script_lines": [
    {"speaker": "host1", "text": "dialogue"},
    {"speaker": "host2", "text": "response"}
  ]
}
Include source citations like [Source p.X] and make it conversational."""
            },
            {
                "role": "user",
                "content": f"""Create conversation for segment: {segment_info.get('title')}

Key points to cover: {segment_info.get('key_points', [])}

Source facts:
{facts_context}

Generate 4-6 exchanges between host1 and host2."""
            }
        ]
        
        response = await self.reasoning_client.generate(messages, response_type="segment")
        
        if response and 'choices' in response:
            script_content = response['choices'][0]['message']['content']
            if isinstance(script_content, dict) and 'script_lines' in script_content:
                return script_content['script_lines']
        
        # Fallback script
        return [
            {"speaker": "host1", "text": f"Let's discuss {segment_info.get('title', 'this topic')}."},
            {"speaker": "host2", "text": "This is a fascinating area of research with significant implications."},
            {"speaker": "host1", "text": "The methodology here is particularly interesting."},
            {"speaker": "host2", "text": "And the results demonstrate clear advances in the field."}
        ]
    
    async def _factcheck_segment(self, paper_id: str, script: List[Dict[str, str]]) -> Dict[str, Any]:
        """Fact-check segment script against source material"""
        logger.info("🔍 Fact-checking segment")
        
        # Extract claims from script
        claims = [line['text'] for line in script]
        
        # Check each claim against RAG
        verified_claims = []
        flagged_claims = []
        
        for i, claim in enumerate(claims):
            # Search for supporting evidence
            supporting_facts = await self.retrieval.retrieve_facts(claim[:100], k=2, paper_id=paper_id)
            
            if supporting_facts:
                verified_claims.append(i)
            elif "[Source" not in claim:  # Flag claims without citations
                flagged_claims.append(i)
        
        factcheck_score = len(verified_claims) / len(claims) if claims else 1.0
        
        return {
            "factcheck_score": factcheck_score,
            "verified_claims": verified_claims,
            "flagged_claims": flagged_claims,
            "needs_rewrite": len(flagged_claims) > 0,
            "total_claims": len(claims)
        }
    
    async def _rewrite_segment(self, paper_id: str, script: List[Dict[str, str]], factcheck: Dict[str, Any]) -> List[Dict[str, str]]:
        """Rewrite segment to address fact-checking issues"""
        logger.info("✏️  Rewriting segment for better factuality")
        
        # Get additional facts for problematic lines
        flagged_indices = factcheck.get('flagged_claims', [])
        
        if not flagged_indices:
            return script
        
        # For simplicity, add source citations to flagged claims
        improved_script = []
        for i, line in enumerate(script):
            if i in flagged_indices:
                # Add a generic citation 
                improved_text = line['text']
                if "[Source" not in improved_text:
                    improved_text += " [Source p.1]"
                improved_script.append({"speaker": line['speaker'], "text": improved_text})
            else:
                improved_script.append(line)
        
        return improved_script
    
    async def _generate_segment_audio(self, paper_id: str, segment_num: int, script: List[Dict[str, str]]) -> str:
        """Generate audio for segment"""
        logger.info(f"🎵 Generating audio for segment {segment_num}")
        
        audio_segments = []
        for line in script:
            audio_segments.append({
                "speaker": line['speaker'],
                "text": line['text'],
                "emotion": "conversational"
            })
        
        audio_path = await self.audio_producer.generate_podcast_audio(
            audio_segments, 
            f"{paper_id}_segment_{segment_num}"
        )
        
        return audio_path
    
    async def _generate_episode_audio(self, paper_id: str, segments: List[Dict[str, Any]]) -> str:
        """Generate final episode audio by combining segments"""
        logger.info("🎶 Generating final episode audio")
        
        # Collect all audio files
        audio_files = []
        for segment in segments:
            if segment.get('audio_path'):
                audio_files.append(segment['audio_path'])
        
        if not audio_files:
            return None
        
        # For now, return the first audio file as placeholder
        # In production, this would stitch all segments together
        return audio_files[0] if audio_files else None
    
    def _calculate_factuality(self, segments: List[Dict[str, Any]]) -> float:
        """Calculate overall factuality score"""
        if not segments:
            return 0.0
        
        total_score = 0
        count = 0
        
        for segment in segments:
            factcheck = segment.get('factcheck', {})
            if 'factcheck_score' in factcheck:
                total_score += factcheck['factcheck_score']
                count += 1
        
        return total_score / count if count > 0 else 0.0


async def run_end_to_end_test(use_local_llm: bool = False):
    """Run complete end-to-end test of the agentic workflow"""
    print("🚀 Starting End-to-End Agentic Workflow Test")
    print("=" * 60)
    
    if use_local_llm:
        print("🦙 Testing with LOCAL LLM (Ollama)")
        print("   Make sure Ollama is running: ollama serve")
        print("   Required models: ollama pull llama3:8b-instruct")
        print("                   ollama pull nomic-embed-text")
    else:
        print("🎭 Testing with MOCK responses")
    
    print("-" * 60)
    
    # Initialize orchestrator
    orchestrator = PodcastAgentOrchestrator(use_local_llm=use_local_llm)
    
    # Test with sample paper
    sample_paper = Path("samples/papers/transformer_attention.txt")
    if not sample_paper.exists():
        print("⚠️  Sample paper not found, creating one...")
        sample_paper.parent.mkdir(parents=True, exist_ok=True)
        with open(sample_paper, 'w') as f:
            f.write("""Attention Is All You Need

Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

Introduction: Recurrent neural networks have been the dominant approach for sequence modeling. However, recurrence precludes parallelization within training examples. 

Methods: The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.

Results: On the WMT 2014 English-to-German translation task, our model achieves 28.4 BLEU, improving over the existing best results by over 2 BLEU.

Conclusion: We showed that the Transformer, a model based entirely on attention, can achieve superior performance on machine translation tasks.""")
    
    # Run the complete workflow
    try:
        episode = await orchestrator.process_paper("transformer-demo", str(sample_paper))
        
        print("\n🎉 End-to-End Test Results:")
        print(f"   📄 Paper: {episode.get('title', 'Unknown')}")
        print(f"   📊 Factuality Score: {episode.get('factuality_score', 0):.2f}")
        print(f"   🎬 Segments Generated: {len(episode.get('segments', []))}")
        print(f"   ⏱️  Total Duration: {episode.get('total_duration', 0)}s")
        print(f"   🎵 Audio Generated: {'✅' if episode.get('audio_path') else '❌'}")
        
        # Show segment details
        print("\n📝 Segment Details:")
        for i, segment in enumerate(episode.get('segments', []), 1):
            factcheck = segment.get('factcheck', {})
            print(f"   {i}. {segment.get('title', 'Untitled')}")
            print(f"      Factcheck: {factcheck.get('factcheck_score', 0):.2f}")
            print(f"      Script lines: {len(segment.get('script', []))}")
            print(f"      Audio: {'✅' if segment.get('audio_path') else '❌'}")
        
        print(f"\n✅ SUCCESS! Complete agentic workflow tested.")
        print(f"📁 Check temp/audio/ for generated audio files")
        
        return episode
        
    except Exception as e:
        print(f"\n❌ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Test the complete agentic workflow")
    parser.add_argument("--local-llm", action="store_true", 
                       help="Use local LLM (Ollama) instead of mocks")
    
    args = parser.parse_args()
    
    # Run the test
    asyncio.run(run_end_to_end_test(use_local_llm=args.local_llm))